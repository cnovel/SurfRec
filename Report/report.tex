\documentclass[a4paper]{article}

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{amsthm}
\usepackage{amssymb}

\title{Surface reconstruction\\
\large{CO512 ISO Report}}

\author{Cyril NOVEL}

\date{\today}

\begin{document}
\maketitle
\newpage

\tableofcontents

\newpage

\section*{Introduction}
\addcontentsline{toc}{section}{Introduction}
In the last decades, numerous devices have been developped in order to capture and digitize our world. Among them, 3D scanners aim at capturing the geometry of the objects surounding us. Statues, monuments but also industrial parts, landscapes : all these can be digitized into a 3D point cloud. But a 3D point cloud isn't enough for a correct visualization by human beings. Thus the reconstruction of the surface is needed to obtain a more comprehensible object.

Because of the numerous fields it covers -- cultural, industrial, medical -- Surface reconstruction is a widely studied subject in Computer Science. In this report, we will present the three main categories of algorithms used to perform surface reconstruction. For each category we will describe two or more precise algorithms.

\newpage

\section{Delaunay-based algorithms}
Delaunay-based algorithms are based on the Delaunay triangulation of a point cloud and the Voronoi diagram of a point cloud. In 2D, a Delaunay triangulation of a set of points $P$ is a triangulation such that no point in $P$ is inside the circumcircle of any triangle of the triangulation. We can generalize Delaunay triangulation in a $n$-dimension space, using simplex. For surface reconstruction, $n = 3$ so that the property becomes : a Delaunay triangulation of a set of points $P$ such that no point in $P$ is inside the circumspeher of any tetrahedron of the triangulation.

A Voronoi diagram is a way to divide the space. Given a set of points $\{p_i\}$,  for each point there will be a corresponding region $R_i = \{x | dist(p_i,x) < dist(p_j,x) ; i \ne j\}$. The regions are called Voronoi cells.

The Voronoi diagram is the dual of the Delaunay triangulation.

\subsection{General idea}
The Delaunay-based algorithm can be divided into two main steps. First, it computes a geometric triangulation of the finite set of points via the Delaunay triangulation or the Voronoi diagram. Then, it extracts a collection of facets chosen so that they are close to the actual surface.

The advantage of Delaunay-based algorithms is that the triangulation created is more robust than other approaches. Delaunay triangulation comes with a variety of theoretical guarantees. However, the Delaunay triangulation is very expensive to compute, thus these algorithms are very expensive for large point clouds.

We describe two different Delaunay based algorithms : the Power Crust algorithm and the Cocone algorithm.

\subsection{Power crust algorithm}
The Powercrust algorithm compute an approximate medial axis of the 3D point cloud $S$ using the Voronoi diagram. We then take a subset $V$ of the Voronoi vertices, called the poles. The poles lie near the medial axis. Each pole has an associated ball, called the polar ball. The polar balls approximate maximal balls contained in the interior or exterior of the 3D shape. The radii of the polar balls are the weights of the poles. The inverse transform is then approximate using the power diagram using the set of weighted poles.

The Power Crust algorithm can be summarize in 6 steps :
\begin{enumerate}
\item Compute the Voronoi diagram of the sample points $S$.
\item For each sample point, compute its poles.
\item Compute the power diagram of the poles.
\item Label each pole either inside or outside.
\item Output the power diagram faces separating the cells of inside and outside poles as the power crust.
\item Output the regular triangulation faces connecting inside poles as the power shape.
\end{enumerate}

The Power crust algorithm uses the notion of the \textit{Medial Axis Transform}. Let $F$ be the boundary of the 3D object. To avoid infinity edges and/or points, we assume that $F$ is contained in a bounded open region $Q$ -- for example a 3D box containing $F$. $F$ divides $Q$ into interior and exterior solids. Let $B_{c,r}$ be a ball of center $c$ and radius $r$. $B$ is empty if the interior of $B$ contains no point of $F$. A medial ball is a maximal empty ball, meaning that no other empty ball can completely contain it. The center of this medial ball is either a center of curvature of $F$ or a point with more than one closest point on $F$.

The \textit{Medial Axis Transform} of $F$ is the set of medial balls of F. The centers of the medial balls form the medial axis of $F$. The medial axis includes a part inside of $F$ and a part outside of $F$. The medial axis of a three-dimensional object is generally a two-dimensional surface.

Assuming a dense sampling, the Voronoi cell of every point of the data set is long, skinny and \textit{perpendicular} to the surface. It happens because in directions tangent to the surface the Voronoi cell is limited by the very close neighbors. So the Voronoi cell extends perpendicularly away from the surface and cannot extend farther than the medial axis. There it is not the closest point anymore -- due to the nature of the medial axis. Thus, the Voronoi vertices at the two ends of the long, skinny Voronoi cell should lie near the median axis. This motivates the selection of poles as an approximation of the median axis.


In pratice, the selection of the poles is done as follow : compute the Voronoi diagram of $S$, with a surrounding box of 8 vertices in order to avoid infinite edges. For each point $s \in S$, find the farthest vertex of the corresponding Voronoi cell, $p_1$. $p_1$ is the first pole of $s$. Then find the farthest vertex $p_2$ of the corresponding cell, such that $p_2.p_1 < 0$. $p_2$ is the second pole of $s$.

The inverse transform uses a \textit{power diagram}. A finite set of balls can be related to power diagrams, which are a kind of Voronoi diagrams, using the distance function :
$$d_{pow}(x, B_{c,r}) = d^2(c,x) - r^2$$
with $d$ the usual distance function in an euclidean space  $B_{c,r}$ the ball of center $c$ and radius $r$. Note that if $x$ is inside $B_{c,r}$, $d_{pow}$ is negative. We use $d_{pow}$ to define a Voronoi diagram, called the \textit{power diagram}.

Considering the power diagram of the polar balls, the \textit{power crust} is the boundary between the power diagrams cells belonging to inner poles and power diagramms cells belonging to outer poles. A facet of the power crust separates cells corresponding to an inner and an outer pole. With a dense sampling, the two polar balls should intersect only slightly, since the inner ball is mostly inside the object and the outer ball outside. The power crust actually interpolates the points of the original point cloud.

The previous paragraph stresses the fact that we need to label the poles as inner poles or outer poles. We define a graph such that two cells are connected in the graph if they share a two-dimensional face. Moreover, two faces are connected of they belong to the poles generated by the same point $s$. When $S$ is well-sampled, two properties can be extracted. The first is that an inner polar ball and an outer polar ball can only intersect shallowly. The second is that every sample as one unique inner pole and one unique outer pole. 

The first way to label the cells -- and therefore the poles -- is via a naive algorithm. It begins by labeling poles adjacent to the points of the bounding box as outer poles and propagates labels as follows. For any pole $p$ labeled outer, if it has an unlabeled neighbor $q$ such that the polar balls of $p$ and $q$ intersect deeply, we label $q$ as outer. Else, we label it as inner. Similarly for the inner poles: deeply intersecting polar balls get labeled as inner, else get labeled outer. The deepness of the intersection is calculated by the angle formed by the 2 tangent planes at the intersetion of the two polar balls.

Unfortunately, the sampling assumption is not true everywhere. If an error is made in the previous labelling algorithm, it propagates and leads to a wrong surface. A solution is to keep track of the belief that an unlabeled ball is inner or outer, based on the labels already assigned. Each ball keeps track of two values between 0 and 1. One value is the belief that the ball is in, the other the belief that the ball is out. If $belief(in) = 1$, we are certain that the ball is an inner ball. If $belief(in) = 0$, the state of the ball is unknown. We initialize all poles close to the bounding box to $belief(out) = 1$ and for all the other balls', $belief(in) = belief(out) = 0$.
All the remaining unlabeled poles are put in a priority queue. The priority is determined by the $belief(in)$ and $belief(out)$ values. If one of these two values is zero, we use the other for the priority queue. If both are non-zero, the pole is \textit{confused} and is assigned priority $|in - out| - 1$, which is between 0 and -1.

Then repeatedly, we remove the top element of the queue and label it inner or outer, depending on the biggest value. We then computes the new belief values for the remaining poles. If $p$ and $q$ are the two poles of a sample point $s$, $\beta$ denotes the angle between $p,s$ and $q,s$. We have $\pi/2 < \beta < \pi$. The bigger $\beta$ is, the more likely it is that $q$ shoud get opposite label from $p$. So we use $0 \le -cos\beta \le 1$ as the weight of the connection between $p$ and $q$. We note $tmp(p)$ the maximum value of $belief(in)(p)$ and $belief(out)(p)$. If $p$ is labeled as inner, then $belief(out)(q) = max(tmp(p)*-cos\beta, belief(out)(q))$.

We know that two balls with different lables intersect shallowly. So the deeper the intersection, the more likely the two balls will have the same label. So we set the weight to be $0 \le -cos\alpha \le 1$. So if $p$ is labeled as inner, then $belief(in)(q) = max(tmp(p)*-cos\alpha, belief(in)(q))$.

We have the two rules for updating the priority queue. We just have to loop until every pole is labeled, and then extract the power crust.

\subsection{Cocone algorithm}

\newpage

\section{Region-growing algorithms}
\subsection{General idea}
A classic region-growing algorithm begins by initiating a triangle as an initial region and then iterates to link new triangles on the region's boundaries. This type of algorithm is very fast. Most of the time, each point of the point cloud is considered only once for the triangulation. The disadvantage is that the reconstruction relies heavily on parameters chosen by the user and on the sampling of the point cloud. Moreover this method can create small holes in the surface when poor data  exists -- due to noise for example. 

We describe two different region-growing algorithms : the Ball Point algorithm and the Fast Reconstruction algorithm of the Point Cloud Library.

\subsection{Ball Point algorithm}
The idea of the BPA is very simple : three points form a triangle if a ball of radius $\rho$ touches them without containing any other points. Starting with a seed triangle, the sphere pivots around an edge of the triangle until it touches another point. It then formed a new triangle. We loop this process until all reachable edges have been considered.

Let $M$ be the surface of the 3D object and $S$ be the sampling points of $M$. For every point $s \in S$, we have the geometric position and the normal orientation. We make a sampling assumption: $S$ is dense enough so that no ball with radius $\rho$ can go through the surface without touching sample points.

% HERE SAMPLING ASSUMPTIONS

The BPA works by finding a seed triangle and use an advancing front to track which points have been visited and which are in the current boundary. The front $F$ is a collection of linked lists of edges. It is initialized by a single loop containing the three edges defined by the seed triangle. Each edge is represented by its two end points, the opposite vertex and the center of the ball that touches all three points. We classify the edge as \textit{active} or \textit{boundary}. An \textit{active} edge will be used for pivoting. If the pivoting is impossible, then we classify it as \textit{boundary}. This information about the states of the edges allows a simpler and clearer algorithm for the pivoting of the ball. We do not use a single linked list because when the ball pivots around an edge, it can reach a new data point or a previously seen data point. Depending on this, the front changes topology. Algorithm~\ref{alg:bpa} shows the pseudocode for the Ball Pivoting Algorithm.

\begin{algorithm}
\caption{Ball Pivoting Algorithm}
\label{alg:bpa}
\begin{algorithmic}[5]
\While{$true$}
  \While{$e_{i,j} = get\_active\_edge(F)$}
    \If{$s_k = ball\_pivot(e_{i,j}) \&\& (not\_used(s_k) \| on\_front(s_k))$}
      \State $output\_triangle(s_i, s_k, s_j)$
      \State $join(e_{i,j}, s_k, F)$
      \If{$e_{k,i} \in F$}
        \State $glue(e_{i,k}, e_{k,i}, F)$
      \EndIf
      \If{$e_{j,k} \in F$}
        \State $glue(e_{k,j}, e_{j,k}, F)$
      \EndIf
    \Else
      \State $mark\_as\_boundary(e_{i,j})$
    \EndIf
  \EndWhile

  \If{$(s_i, s_j, s_k) = find_seed_triangle()$}
    \State $output\_triangle(s_i, s_j, s_k)$
    \State $insert\_edge(e_{i,j}, F)$
    \State $insert\_edge(e_{j,k}, F)$
    \State $insert\_edge(e_{k,i}, F)$
  \Else
    \State $return$
  \EndIf
\EndWhile
\end{algorithmic}
\end{algorithm} 

Two functions -- $ball\_pivot$ and $find\_seed\_triangle$ -- need an efficient way to retrieve the neighbourhood of a point. A simple solution is using a grid of small cubes, called voxels. The voxel's side is $\delta = 2\rho$. We store the data points in a list for each voxels. Given a point $s$, we can find the voxel $v$ it lies in by divided its coordinates by $\delta$. In order to look at every point that lies at a distance $\rho$ of $s$, we need to look at all the neighbours of $v$. Hence a total of 27 voxels to consider.

The selection of a seed is necessary for the initialisation of the pivoting. A seed triangle is a triangle such that a $\rho$ ball with its center in the outward space touches the three points of the seed triangle and contains no other data points. A way to find a seed triangle is :
\begin{itemize}
\item Pick any point $s$ not yet used by the triangulation;
\item Consider all pairs of point $s_1$, $s_2$ in the 27 voxels around $s$;
\item Build triangle $s_1$,$s_2$,$s$;
\item Orient the triangle normal thanks to the point normals;
\item Test that a $\rho$ ball with its center in the outward space touches the three points of the seed triangle and contains no other data points;
\item Stop when a valid seed triangle has been found.
\end{itemize}

The second step can be refined by selecting $s_1$ and $s_2$ such that they are at a distance less than $\rho$ from $s$. We can limit ourselves to one point per voxel as candidate vertex for the seed triangle. Also, for a given voxel we can average the normal of all the points, which approximates the surface in that region. Then we consider first points $s$ such that the projection of their normals on the average normal is largely positive.

The pivoting operation starts with a triangle $t =(s_i, s_j, s_0)$ and a $\rho$ ball that touches its three vertices. Let $e_{i,j}$ be the pivoting edge. The ball initially contains no other points than the points of $t$. Let $c_{i,j,0}$ be the center of the initial ball. When we are pivoting the ball around $e_{i,j}$, the center $c_{i,j,0}$ describes a circle which lies in the plan perpendicular to $e_{i,j}$ and passing by the midpoint $m = 1/2 * (s_i + s_j)$. The center of the circular trajectory is $m$ and the radius is $\| c_{i,j,0} - m \|$. If during the pivoting we hit a new point $s_k$, the triangle $(s_i, s_k, s_j)$ is a new valid triangle and the ball contains no other points, so it is valid for the next pivoting step. If we don't hit a new point, then $e_{i,j}$ is marked as boundary.
In pratice, we consider all points in the 27 voxels around $m$. For each point $s_x$, we compute -- if possible -- the center $c_x$ of the $\rho$ ball that touches $s_x$, $s_i$ and $s_j$. $c_x$ lies on the circular trajectory describes previously. Among all the $c_x$ points, we select the one that comes first along this trajectory -- via a simple angle computation. We now have the new triangle and the new center of the ball.

Once the new triangle is find, we have to update the front information. The \textit{join} operation is used when the ball pivot around $e_{i,j}$ and touches a \textit{not\_used} vertex $s_k$. In that case, we output the triangle and locally modify the front by removing $e_{i,j}$ and adding $e_{i,k}$ and $e_{k,j}$. If $s_k$ is already part of the mesh, two possibilities arise:
\begin{enumerate}
\item $s_k$ doesn't belong to the front. We don't generate the triangle since it would produce a non manifold mesh. Here $e_{i,j}$ is marked as boundary edge.
\item $s_k$ belongs to the front. We check edge orientation to orient correctly the ouput triangle, then we apply the \textit{join} operation. This operation can create one or two pairs of coincident edges.
\end{enumerate}

The coincident edges are removed by the \textit{glue} operation. Coincident egdes always have opposite orientation. For example, when edge $e_{i,k}$ is added by \textit{join}, if $e_{k,i}$ is on the front, \textit{glue} will remove the two edges. Two cases arise:
\begin{enumerate}
\item $e_{i,k}$ and $e_{k,i}$ are in the same linked list. We just have to removed them, since they are consecutive in the linked list.
\item $e_{i,k}$ and $e_{k,i}$ are in different linked lists, respectively $l_1$ and $l_2$. We remove $e_{i,k}$ from $l_1$ and called $l_{1,start}$ the first part of $l_1$ and $l_{1,end}$ the last part of $l_1$. We do the same thing with $l_2$. Then $l_1 = l_{1,start}+l_{2,end}$ and $l_2 = l_{2,start}+l_{1,end}$.
\end{enumerate}


\subsection{Fast Reconstruction algorithm (PCL)}
The Point Cloud Library is an open-source library for processing point clouds. It provides a greedy algorithm for reconstructing a surface from an unorganized point cloud as fast as possible. The idea is simple: for each point $s$, project its neighbourhood on the average plane, add edges according to visibility and go back to the 3D space. The algorithm is largely inspired by %tofill

This algorithm requires 2 parameters : $\mu > 1$ and $\alpha$. $\mu$ quantifies the idea of locally uniform sampling. $\alpha$ gives an upper bound on the angle between consecutive neighbors of a point on a boundary surface. $\alpha$ is a large angle, set at 120 degrees. The data points are categorized as \textit{free}, \textit{fringe}, \textit{boundary} or\textit{completed}. \textit{Free} points have no incident triangles. Points on the current boundary are labeled as \textit{fringe} or \textit{boundary}: a \textit{fringe} point has already been processed but have some missing triangles around it due to $\alpha$; a \textit{boundary} point has not yet been processed. The algorithm keep two properties true during its execution :
\begin{itemize}
\item No \textit{free}, \textit{fringe} or \textit{boundary} point can be in the interior of a triangle.
\item At the end of each iteration, the point processed is either labeled \textit{completed} or \textit{boundary}.
\end{itemize}

The neighbourhood of $s$ is defined by all the points that are in the sphere centered at $s$ and of radius $\mu m$ where $m$ is the distance between $s$ and its closest neighbour. This sphere is called sphere of influence around $s$. The selected points are called the candidate points and the set is noted $C_s$. Note that a point $p$ can be in the sphere of influence of $s$ and $s$ doesn't lie in the sphere of influence of $p$. However this asymmetry doesn't have an impact on the toplogy of the reconstructed mesh. Different techniques can be used to find the points in the sphere of influence. We can use an octree structure, a voxel grid or a kd-tree.

The triangulation around $s$ is made on the projection plane $P_s$. The choice of $P_r$ is crucial for the robustness of the algorithm. The best plane would be the tangent plane of the actual surface, but it is very expensive to compute -- since we are looking for the surface. A fast approach can be averaging the normals of the existing triangles incident to $s$. Another solution is to compute the least square plane with the covariance matrix of the points in the sphere. Once the plane is found, we project the points in $C_s$ on the plane and called the resulting set of points $C^p_s$.

The next step is to order the points in $C^p_s$ around $s$. We define the coordinate system of $P_s$ with $s$ as the origin. Each point is characterized by the angle $\theta$ between the $x$ axis and the vector from $s$ to the point. Then $C^p_s$ is partitioned by the quadrants in which they lie. In each quadrant, we sort the points based on $sin^2(x)$. These ordered sets are merged and thus we have the angle ordering around $s$.

Among the points in $C^p_s$, some points are not visible from $s$. We have to remove them from the set before performing the triangulation step. We define a \textit{boundary edge} as any edge with only one triangle incident on it. The point of a \textit{boundary edge} therefore have to be labeled as \textit{fringe} or \textit{boundary}. \textit{Internal edges} are edges connecting a \textit{completed} point to any other points. The \textit{boundary} edges are also projected on $P_s$. If the line of sight from $s$ to a point in $C^p_s$ is obstructed by an edge, then the point is occluded and removed from $C^p_r$. All the points between consecutive \textit{boundary edges} of $s$ are removed as the can't be seen from $s$. Then we can remove \textit{fringe} or \textit{boundary} that have $s$ in their invisible region. Once this is done, we test the remaining points for occlusion. The naive approach of considering all the edges for occlusion is very expensive. The following theorem limits the search to few edges:
\newtheorem{mydef}{Theorem}
\begin{mydef}
Only the \emph{boundary edges} of the points in $C_s$ can be possible occluding edges between $s$ and $C^p_s$.
\end{mydef}

The remaining points are ordered and stored in $C^f_s$. Then we perform the triangulation step. The points in $C^f_s$ are connected to $s$. If the angle between two consecutive points is less than $\alpha$, the two points are connected with a \emph{boundary edge}. If an edge already exists, then we label it as an \emph{internal edge}. If the angle is greater than $\alpha$, then the two points are not connected to each others. We label the edges starting from $s$ accordingly to the triangles formed. If $s$ is surrounded by triangles, $s$ is labeled as \emph{completed}. Else $s$ is labeled as \emph{boundary}. All the \emph{free} points in $C^f_s$ are labeled as \emph{fringe} points and are append to the queue of points to process.

\newpage

\section{Implicit surface algorithms}
\subsection{General idea}
In an implicit surface algorithm, a signed distance function is defined from the sample points and computed. Then given the zero-set of the signed distance function, an approximate triangulated surface is constructed.

There is a major difference between Implicit surface algorithms and Region-growing or Delaunay-based algorithms. Region-growing and Delaunay based methods interpolate sample points, mean that the reconstructed surface lies on the sample points. The implicit surface approach approximates sample points and can lack accuracy. But one advantage is that implicit surface methods are ideal in the case of highly noised data.

We describe four different implicit surface algorithms : the Poisson algorithm, the Radial Basis Function algorithm, the Level Set algorithm and the Marching Cubes algorithm.


\subsection{Poisson algorithm}
In the Poisson algorithm, we compute an \emph{indicator function} $\chi$, defined as 1 at points inside the 3D object, 0 otherwise. We obtain the reconstructed surface by extracting the right isosurface. The main idea is that there is a relationship between the sampled oriented points and the indicator function of the model. The gradient of $\chi$ is  a vector field that is zero almost everywhere, since $\chi$ is constant almost everywhere. At points near the surface however, the gradient of $\chi$ is equal to the inward surface normals. So the oriented samples can be viewed as samples of the gradient of $\chi$.

The goal is to find the function $\chi$ whose gradient best approximates the vector field $\vec{V}$ defined by the given oriented points. So we have to solve $min_\chi\|\nabla\chi - \vec{V}\|$. By applying the divergence operator, this problem becomes a Poisson problem: find $\chi$ whose Laplacian -- divergence of gradient -- equals the divergence of $\vec{V}$:
$$\Delta\chi = div(\vec{V})$$

Let $S$ be the input data, with $s \in S$ a sample consisting of $s.p$ the position and $s.\vec{N}$ the inward facing normal. We assume $s$ is lying on or near the surface $\partial M$ of an unknow model $M$.

Because $\chi$ is a piecewise constant function, its explicit gradient field would result in a vector field with unbounded values at the surface boundary. To avoid this problem, $\chi$ is convolve with a smoothing filter and we consider the gradient field of the smoothing function.

\newtheorem{lemma}{Lemma}
\begin{lemma}
Given $M$ and its boundary $\partial M$, let $\chi_M$ be the indicator function of $M$, $\vec{N}_{\partial M}(p)$ be the inward facing normal at $p \in \partial M$, $\tilde{F}(q)$ be a smoothing filter and $\tilde{F}_p(q) = \tilde{F}(q-p)$ its translation to the point $p$. The gradient of the smmoothed indicator function is equal to the vector field obtained by smoothing the surface normal field:
$$\nabla(\chi_M * \tilde{F})(q) = \int_{\partial M} \tilde{F}_p(q) \vec{N}_{\partial M}(p) dp$$
\end{lemma}

The problem is that we don't know what the surface $\partial M$ is and thus we can't perform the integration. We approximate the gradient field with a discrete summation on the sample points. We use them to sample $\partial M$ into distinct patches $P_s \subset \partial M$ and approximate the integral:
$$\nabla(\chi_M * \tilde{F})(q) \approx \sum_{s \in S} \\|P_s\\| \tilde{F}_{s.p}(q) s.\vec{N} \equiv \vec{V}(q)$$

The choice of the smoothing filter is in pratice important. On the one hand, it should be sufficiently narrow so that the data is not oversmooth. And on the other hand it should be wide enough so that the integral over $P_s$ is well approximated by the value at $s.p$ times the patch area. A good choice of filter that balances these two requirements is a Gaussian filter whose variance is on the order of the sampling resolution.

For the implementation, we must discretize the problem. The idea here is to use a set of function to represent $\chi$ as a combination of those functions. A straightforward approach is to discretize the space with a regular 3D grid. But this uniform structure becomes impractical for fine detail reconstruction. Moreover, some areas are finely details whereas there is no points in them.

We only need an accurate representation of the implicit function near the surface. This motivates the use of an adaptative octree to represent the implicit function and to solve the Poisson system. An octree is a tree data structure in which each node has exactly eight children. In 3D space, each node is a cube, and its children are smaller cubes, dividing the node into 8 equal cubes. We use the positions of the sample points to define the octree $O$ and associate a function $F_n$ to every node $n \in O$. The following conditions have to be satisfied:
\begin{enumerate}
\item $\vec{V}$ can be precisely and efficiently represented as the linear sum of the $F_n$.
\item The matrix representation of the Poisson equation, expressed in terms of the $F_n$ can be solved efficiently.
\item A representation of the indicator function as the sum of the $F_n$ can be precisely and efficiently evaluated near the surface model.
\end{enumerate}

Given $S$ and a maximum tree depth $D$, We define the octree $O$ to be the minimal octree such that every point sample fall into a leaf node at depth $D$. We then define a set of functions obtain as the span of a fixed, unit-integral base function $F: \mathbb{R}^3 \to \mathbb{R}$. For every node $n$, we define $F_n$:
$$F_n(q) = F(\frac{q - n.c}{n.w})\frac{1}{o.w^3}$$
where $n.c$ is the center of the node and $n.w$ is the width of the node.

The space of functions $\{F_n\}$ has a multiresolution structure: deeper nodes are associated with higher frequency functions. Due to the octree representation, the function representation becomes more precise as we are closer to the surface.

%% SELECTION OF THE BASE FUNCTION %%

To obtain a better precision, we use trilinear interpolation to distribute the sample accross the eight nearest node. We define the approximate gradient field of the indicator function as:
$$\vec{V}(q) \equiv \sum_{s \in S} \sum_{n \in Ngbr_D(s)} \alpha_{n,s}F_n(q)s.\vec{N}$$
where $Ngbr_D(s)$ are the 8 depth-D nodes closest to $s.p$ and $\alpha_{n,s}$ is the trilinear interpolation weight define by $\alpha_{n,s} = (1 - \mid{}x_{n.c} - x_s \mid)(1 - \mid y_{n.c} - y_s\mid)(1 - \mid z_{n.c} - z_s\mid)$.

Considering a uniform sampling of the surface, we can assume that $\\|P_s\mid$ is constant and thus that $\vec{V}$ is a good approximation of the gradient of $\chi$, up to a multiplicative constant.

Now that $\vec{V}$ is computed, we want to find a solution $\tilde{\chi} \in \{F_n\}$ such that $\tilde{\chi}$ is a solution to the Poisson equation $\Delta\tilde{\chi} = div{\vec{V}}$. But while $\tilde{\chi}$ and $\vec{V}$ are in $\{F_n\}$, this is not necessarily true for $\Delta\tilde{\chi}$ and $div{\vec{V}}$. So we project $\Delta\tilde{\chi}$ and $div{\vec{V}}$ onto the space $\{F_n\}$ and we choose $\chi$ so that the projection of $\Delta\tilde{\chi}$ is the closest to the projection of $div{\vec{V}}$. In general the functions $F_n$ don't form an orthonormal basis. So solving this problem directly is very computationally expensive. We simplify the problem by solving for the function $\tilde{\chi}$ minimizing:
$$\sum_{n \in O}\|\langle\Delta\tilde{\chi}-div(\vec{V}),F_n\rangle\|^2 = \sum_{n \in O}\|\langle\Delta\tilde{\chi},F_n\rangle - \langle div(\vec{V}), F_n\rangle\|^2$$
So having the $\mid O \mid$-dimensional vector $\nu$ whose $n$-th coordinate is $\nu_n = \langle div(\vec{V}), F_n\rangle$, the goal is to find the coordinates of the projection of the laplacian of $\chi$ so that we found the closest projection as possible. Let $\tilde{\chi} = \sum_n x_n F_n$. We define the $\mid O \mid$ matrix $L$ such that $Lx$ returns the dot product of the Laplacian with each of the $F_n$. $L$ is defined like this:
$$L_{n,n'} = \langle\frac{\partial^2F_n}{\partial x^2}, F_n\rangle + \langle\frac{\partial^2F_n}{\partial y^2}, F_n\rangle + \langle\frac{\partial^2F_n}{\partial z^2}, F_n\rangle$$ 

So we need do find $x$ solution of $\min_{x}\|Lx - \nu\|^2$.

$L$ is sparce because the $F_n$ are compactly supported, and symmetric since $\int f''g = -\int f'g'$.

Once we have $x$, we know the surface indicator function $\tilde{\chi}$. We now have to determine the right isosurface to extract. To select the isovalue, we evaluate $\tilde{\chi}$ at the sample positions and use the average of the values for extracting the isosurface. Thus:
$$\partial\tilde{M} = \{q \in \mathbb{R}^3 \mid \tilde{\chi}(q) = \gamma\} \text{ with } \gamma = \frac{1}{\mid S\mid}\sum_{s \in S}\tilde{\chi}(s.p)$$
This choice for the isovalue has the property that scaling $\tilde{\chi}$ doesn't change the isosurface. Thus, knowing $\vec{V}$ up to a multiplicative constant provides sufficient information for reconstructing the surface.

We know have the isosurface of $S$. If we want a triangulation of the surface, we can use the Marching Cubes algorithm describe in section ??.

\subsection{Radial Basis Function algorithm}

\subsection{Level Set algorithm}

\subsection{Marching cubes algorithm}


\section*{Conclusion}
\addcontentsline{toc}{section}{Conclusion}
\end{document}
